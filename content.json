{"meta":{"title":"Ethanli 的技术屋","subtitle":"","description":"","author":"Ethanli","url":"http://chanforg.github.io","root":"/"},"pages":[{"title":"about","date":"2022-04-06T09:29:23.000Z","updated":"2022-04-06T11:26:59.030Z","comments":false,"path":"about/index.html","permalink":"http://chanforg.github.io/about/index.html","excerpt":"","text":""}],"posts":[{"title":"马尔科夫决策过程","slug":"马尔科夫决策过程","date":"2022-04-06T13:42:10.000Z","updated":"2022-04-06T14:25:00.637Z","comments":true,"path":"2022/04/06/马尔科夫决策过程/","link":"","permalink":"http://chanforg.github.io/2022/04/06/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/","excerpt":"","text":"马尔可夫过程（Markov Process,MP） 马尔可夫特征 一个状态的下一个状态只取决于当前的状态，而与当前状态之前的状态没有关系，那么这个状态转移的过程就符合马尔科夫的。 ### 马尔科夫链与状态转移矩阵 马尔科夫链如下图所示 图中有四种状态 \\(s_1,s_2,s_3,s_4\\),这四种状态之间相互转移，拿\\(s_1\\)来说： \\(s_1\\)继续保持在原状态的概率为0.1 \\(s_1\\)转移到\\(s_2\\)的概率为0.2 \\(s_1\\)转移到\\(s_4\\)的概率为0.7 所以可以用一个矩阵来描述上图： \\[ P=\\left[ \\begin{matrix} \\begin{array}{c} P\\left( s_1|s_1 \\right)\\\\ P\\left( s_1|s_2 \\right)\\\\ P\\left( s_1|s_3 \\right)\\\\ P\\left( s_1|s_4 \\right)\\\\ \\end{array}&amp; \\begin{array}{c} P\\left( s_2|s_1 \\right)\\\\ P\\left( s_2|s_2 \\right)\\\\ P\\left( s_2|s_3 \\right)\\\\ P\\left( s_2|s_4 \\right)\\\\ \\end{array}&amp; \\begin{array}{c} P\\left( s_3|s_1 \\right)\\\\ P\\left( s_3|s_2 \\right)\\\\ P\\left( s_3|s_3 \\right)\\\\ P\\left( s_3|s_4 \\right)\\\\ \\end{array}&amp; \\begin{array}{c} P\\left( s_4|s_1 \\right)\\\\ P\\left( s_4|s_2 \\right)\\\\ P\\left( s_4|s_3 \\right)\\\\ P\\left( s_4|s_4 \\right)\\\\ \\end{array}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix} 0.1&amp; 0.2&amp; 0&amp; 0.7\\\\ 1&amp; 0&amp; 0&amp; 0\\\\ 0&amp; 1&amp; 0&amp; 0\\\\ 0&amp; 0.3&amp; 0.2&amp; 0.5\\\\ \\end{matrix} \\right] \\tag{1} \\] \\((1)\\) 就称为状态转移矩阵，矩阵的每一行都描述了当到达某个节点后，在下一步转移到其他节点的概率。 ### 马尔可夫 设状态的历史为\\(h_{t}=\\left\\{s_{1}, s_{2}, s_{3}, \\ldots, s_{t}\\right\\}\\)(\\(h_t\\)包含了之前的所有状态),如果一个状态转移是符合马尔可夫的，也就是满足如下条件： \\[ p\\left(s_{t+1} \\mid s_{t}\\right) =p\\left(s_{t+1} \\mid h_{t}\\right) \\tag{2} \\] \\[ p(s_{t+1}∣s_t,a_t)=p(s_{t+1}∣h_t,a_t) \\tag{3} \\] 公式\\((2)\\)表示从当前状态转移到下一个状态和从历史的状态转移到下一个状态时等价的，因此就可以认为下一个状态只跟当前状态有关，跟除当前状态外的其他历史状态无关。公式\\((3)\\)是等价的，只是增加了动作，在之后会涉及到。马尔可夫性质是所有的马尔可夫过程的基础。 马尔可夫奖励过程（Markov Reward Processes,MRP） 马尔可夫决策过程（Markov Decision Process ,MDP)","categories":[{"name":"知识库","slug":"知识库","permalink":"http://chanforg.github.io/categories/%E7%9F%A5%E8%AF%86%E5%BA%93/"}],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://chanforg.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"}]},{"title":"强化学习基本概念","slug":"强化学习基本概念","date":"2022-04-06T12:12:41.000Z","updated":"2022-04-06T13:39:27.559Z","comments":true,"path":"2022/04/06/强化学习基本概念/","link":"","permalink":"http://chanforg.github.io/2022/04/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"基础概念 keywords 强化学习（Reinforcement Learning）：Agent可以在与复杂且不确定的Environment进行交互时，尝试使所获得的Reward最大化的计算算法。 Action: Environment接收到的Agent当前状态的输出。 State：Agent从Environment中获取到的状态。 Reward：Agent从Environment中获取的反馈信号，这个信号指定了Agent在某一步采取了某个策略以后是否得到奖励。 Exploration：在当前的情况下，继续尝试新的Action，其有可能会使你得到更高的这个奖励，也有可能使你一无所有。 Exploitation：在当前的情况下，继续尝试已知的可以获得最大Reward的过程，即重复执行这个 Action 就可以了。 深度强化学习（Deep Reinforcement Learning）：不需要手工设计特征，仅需要输入State让系统直接输出Action的一个end-to-end training的强化学习方法。通常使用神经网络来拟合 value function 或者 policy network。 Full observability、fully observed和partially observed：当Agent的状态跟Environment的状态等价的时候，我们就说现在Environment是full observability（全部可观测），当Agent能够观察到Environment的所有状态时，我们称这个环境是fully observed（完全可观测）。一般我们的Agent不能观察到Environment的所有状态时，我们称这个环境是partially observed（部分可观测）。 POMDP（Partially Observable Markov Decision Processes）：部分可观测马尔可夫决策过程，即马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 ss，只能知道部分观测值 oo。 Action space（discrete action spaces and continuous action spaces）：在给定的Environment中，有效动作的集合经常被称为动作空间（Action space），Agent的动作数量是有限的动作空间为离散动作空间（discrete action spaces），反之，称为连续动作空间（continuous action spaces）。 policy-based（基于策略的）：Agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。 valued-based（基于价值的）：Agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。 model-based（有模型结构）：Agent通过学习状态的转移来采取措施。 model-free（无模型结构）：Agent没有去直接估计状态的转移，也没有得到Environment的具体转移变量。它通过学习 value function 和 policy function 进行决策。 序列决策过程（Sequential Decision Making) 在跟环境的交互过程中，agent 会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。所以历史是观测(observation) O、行为 A、奖励R的序列： \\[ H_t=O_1,R_1,A_1,\\cdots ,O_{t-1},R_{t-1},A_{t-1},O_t,A_t \\] Agent 在采取当前动作的时候会依赖于它之前得到的这个历史，所以你可以把整个游戏的状态看成关于这个历史的函数： \\[ S_t=f\\left( H_t \\right) \\] 状态(state) s和观测(observation)o的关系： 状态是对世界的完整描述，不会隐藏任何信息；观测是智能体对于世界的观察，可能会遗漏一些信息 环境有自己的函数 \\(S_{t}^{e}=f^{e}\\left(H_{t}\\right)\\) 来更新状态，在 agent 的内部也有一个函数 \\(S_{t}^{a}=f^{a}\\left(H_{t}\\right)\\) 来更新状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是 full observability，就是全部可以观测。换句话说，当 agent 能够观察到环境的所有状态时，我们称这个环境是完全可观测的(fully observed)。在这种情况下面，强化学习通常被建模成一个 Markov decision process(MDP)的问题。在 MDP 中， \\(O_{t}=S_{t}^{e}=S_{t}^{a}\\)。 但是有一种情况是 agent 得到的观测并不能包含环境运作的所有状态，因为在这个强化学习的设定里面，环境的状态才是真正的所有状态。也就是说当 agent 只能看到部分的观测，我们就称这个环境是部分可观测的(partially observed)。在这种情况下面，强化学习通常被建模成一个 POMDP 的问题。部分可观测马尔可夫决策过程(Partially Observable Markov Decision Processes, POMDP) 是一个马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 s，只能知道部分观测值 o。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。 更多信息请点击","categories":[{"name":"知识库","slug":"知识库","permalink":"http://chanforg.github.io/categories/%E7%9F%A5%E8%AF%86%E5%BA%93/"}],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://chanforg.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-04-06T02:51:39.148Z","updated":"2022-04-06T02:51:39.148Z","comments":true,"path":"2022/04/06/hello-world/","link":"","permalink":"http://chanforg.github.io/2022/04/06/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"知识库","slug":"知识库","permalink":"http://chanforg.github.io/categories/%E7%9F%A5%E8%AF%86%E5%BA%93/"}],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://chanforg.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"}]}
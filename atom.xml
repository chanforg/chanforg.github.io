<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ethanli 的技术屋</title>
  
  
  <link href="http://chanforg.github.io/atom.xml" rel="self"/>
  
  <link href="http://chanforg.github.io/"/>
  <updated>2022-04-06T13:25:55.472Z</updated>
  <id>http://chanforg.github.io/</id>
  
  <author>
    <name>Ethanli</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>强化学习基本概念</title>
    <link href="http://chanforg.github.io/2022/04/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://chanforg.github.io/2022/04/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</id>
    <published>2022-04-06T12:12:41.000Z</published>
    <updated>2022-04-06T13:25:55.472Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基础概念">基础概念</h2><div data-align="center"><p><img src="https://datawhalechina.github.io/easy-rl/chapter1/img/1.1.png" width="50%"></p></div><h3 id="keywords">keywords</h3><ul><li>强化学习（ReinforcementLearning）：Agent可以在与复杂且不确定的Environment进行交互时，尝试使所获得的Reward最大化的计算算法。</li><li>Action: Environment接收到的Agent当前状态的输出。</li><li>State：Agent从Environment中获取到的状态。</li><li>Reward：Agent从Environment中获取的反馈信号，这个信号指定了Agent在某一步采取了某个策略以后是否得到奖励。</li><li>Exploration：在当前的情况下，继续尝试新的Action，其有可能会使你得到更高的这个奖励，也有可能使你一无所有。</li><li>Exploitation：在当前的情况下，继续尝试已知的可以获得最大Reward的过程，即重复执行这个Action 就可以了。</li><li>深度强化学习（Deep ReinforcementLearning）：不需要手工设计特征，仅需要输入State让系统直接输出Action的一个end-to-endtraining的强化学习方法。通常使用神经网络来拟合 value function 或者policy network。</li><li>Full observability、fully observed和partiallyobserved：当Agent的状态跟Environment的状态等价的时候，我们就说现在Environment是fullobservability（全部可观测），当Agent能够观察到Environment的所有状态时，我们称这个环境是fullyobserved（完全可观测）。一般我们的Agent不能观察到Environment的所有状态时，我们称这个环境是partiallyobserved（部分可观测）。</li><li>POMDP（Partially Observable Markov DecisionProcesses）：部分可观测马尔可夫决策过程，即马尔可夫决策过程的泛化。POMDP依然具有马尔可夫性质，但是假设智能体无法感知环境的状态ss，只能知道部分观测值 oo。</li><li>Action space（discrete action spaces and continuous actionspaces）：在给定的Environment中，有效动作的集合经常被称为动作空间（Actionspace），Agent的动作数量是有限的动作空间为离散动作空间（discrete actionspaces），反之，称为连续动作空间（continuous action spaces）。</li><li>policy-based（基于策略的）：Agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。</li><li>valued-based（基于价值的）：Agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。</li><li>model-based（有模型结构）：Agent通过学习状态的转移来采取措施。</li><li>model-free（无模型结构）：Agent没有去直接估计状态的转移，也没有得到Environment的具体转移变量。它通过学习value function 和 policy function 进行决策。</li></ul><h3 id="序列决策过程sequential-decision-making">序列决策过程（SequentialDecision Making)</h3><p>在跟环境的交互过程中，agent会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。<strong>所以历史是观测(observation)O、行为 A、奖励R的序列</strong>： <span class="math display">\[H_t=O_1,R_1,A_1,\cdots ,O_{t-1},R_{t-1},A_{t-1},O_t,A_t\]</span> Agent在采取当前动作的时候会依赖于它之前得到的这个历史，所以你可以把整个游戏的状态看成关于这个历史的函数：<span class="math display">\[S_t=f\left( H_t \right)\]</span> <span class="label label-primary">状态(state)</span><strong>s</strong>和<span class="label label-primary">观测(observation)</span><strong>o</strong>的关系：</p><ul><li>状态是对世界的完整描述，不会隐藏任何信息；观测是智能体对于世界的观察，可能会遗漏一些信息</li><li>环境有自己的函数 <spanclass="math inline">\(S_{t}^{e}=f^{e}\left(H_{t}\right)\)</span>来更新状态，在 agent 的内部也有一个函数 <spanclass="math inline">\(S_{t}^{a}=f^{a}\left(H_{t}\right)\)</span>来更新状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是<strong>full observability</strong>，就是全部可以观测。换句话说，当agent能够观察到环境的所有状态时，我们称这个环境是<span class="label label-primary">完全可观测的(fully observed)</span>。在这种情况下面，强化学习通常被建模成一个<strong>Markov decision process(MDP)</strong>的问题。在 MDP 中， <spanclass="math inline">\(O_{t}=S_{t}^{e}=S_{t}^{a}\)</span>。</li><li>但是有一种情况是 agent得到的观测并不能包含环境运作的所有状态，因为在这个强化学习的设定里面，环境的状态才是真正的所有状态。也就是说当agent只能看到部分的观测，我们就称这个环境是<span class="label label-primary">部分可观测的(partially observed)</span>。在这种情况下面，强化学习通常被建模成一个POMDP 的问题。<strong>部分可观测马尔可夫决策过程(Partially ObservableMarkov Decision Processes, POMDP)</strong>是一个马尔可夫决策过程的泛化。POMDP依然具有马尔可夫性质，但是假设智能体无法感知环境的状态s，只能知道部分观测值o。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。</li></ul><p>更多信息<ahref="https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1">请点击</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基础概念&quot;&gt;基础概念&lt;/h2&gt;
&lt;div data-align=&quot;center&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://datawhalechina.github.io/easy-rl/chapter1/img/1.1.png&quot; width=&quot;50%&quot;&gt;</summary>
      
    
    
    
    <category term="强化学习" scheme="http://chanforg.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="强化学习" scheme="http://chanforg.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://chanforg.github.io/2022/04/06/hello-world/"/>
    <id>http://chanforg.github.io/2022/04/06/hello-world/</id>
    <published>2022-04-06T02:51:39.148Z</published>
    <updated>2022-04-06T02:51:39.148Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></div></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo server<br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo generate<br></code></pre></div></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></div></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very
first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; fo</summary>
      
    
    
    
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ethanli 的技术屋</title>
  
  
  <link href="http://chanforg.github.io/atom.xml" rel="self"/>
  
  <link href="http://chanforg.github.io/"/>
  <updated>2022-04-07T06:19:38.540Z</updated>
  <id>http://chanforg.github.io/</id>
  
  <author>
    <name>Ethanli</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>马尔科夫决策过程</title>
    <link href="http://chanforg.github.io/2022/04/06/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
    <id>http://chanforg.github.io/2022/04/06/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</id>
    <published>2022-04-06T13:42:10.000Z</published>
    <updated>2022-04-07T06:19:38.540Z</updated>
    
    <content type="html"><![CDATA[<h1 id="马尔可夫过程（Markov-Process-MP）"><a href="#马尔可夫过程（Markov-Process-MP）" class="headerlink" title="马尔可夫过程（Markov Process,MP）"></a>马尔可夫过程（Markov Process,MP）</h1><h2 id="马尔可夫特征"><a href="#马尔可夫特征" class="headerlink" title="马尔可夫特征"></a>马尔可夫特征</h2><p>一个状态的下一个状态只取决于当前的状态，而与当前状态之前的状态没有关系，那么这个状态转移的过程就符合<strong>马尔科夫</strong>的。    </p><h3 id="马尔科夫链与状态转移矩阵"><a href="#马尔科夫链与状态转移矩阵" class="headerlink" title="马尔科夫链与状态转移矩阵"></a>马尔科夫链与状态转移矩阵</h3><p><strong>马尔科夫链</strong>如下图所示</p><div align="center"><img src="https://datawhalechina.github.io/easy-rl/chapter2/img/2.5.png" width="50%">       </div> <p>图中有四种状态 $s_1,s_2,s_3,s_4$,这四种状态之间相互转移，拿$s_1$来说：</p><ul><li>$s_1$继续保持在原状态的概率为0.1</li><li>$s_1$转移到$s_2$的概率为0.2</li><li>$s_1$转移到$s_4$的概率为0.7</li></ul><p>所以可以用一个矩阵来描述上图：   </p><p>$$<br>P&#x3D;\left[ \begin{matrix}<br>    \begin{array}{c}<br>    P\left( s_1|s_1 \right)\<br>    P\left( s_1|s_2 \right)\<br>    P\left( s_1|s_3 \right)\<br>    P\left( s_1|s_4 \right)\<br>\end{array}&amp;        \begin{array}{c}<br>    P\left( s_2|s_1 \right)\<br>    P\left( s_2|s_2 \right)\<br>    P\left( s_2|s_3 \right)\<br>    P\left( s_2|s_4 \right)\<br>\end{array}&amp;        \begin{array}{c}<br>    P\left( s_3|s_1 \right)\<br>    P\left( s_3|s_2 \right)\<br>    P\left( s_3|s_3 \right)\<br>    P\left( s_3|s_4 \right)\<br>\end{array}&amp;        \begin{array}{c}<br>    P\left( s_4|s_1 \right)\<br>    P\left( s_4|s_2 \right)\<br>    P\left( s_4|s_3 \right)\<br>    P\left( s_4|s_4 \right)\<br>\end{array}\<br>\end{matrix} \right] &#x3D;\left[ \begin{matrix}<br>    0.1&amp;        0.2&amp;        0&amp;        0.7\<br>    1&amp;        0&amp;        0&amp;        0\<br>    0&amp;        1&amp;        0&amp;        0\<br>    0&amp;        0.3&amp;        0.2&amp;        0.5\<br>\end{matrix} \right] \tag{1}<br>$$  </p><p>$(1)$ 就称为<strong>状态转移矩阵</strong>，矩阵的每一行都描述了当到达某个节点后，在下一步转移到其他节点的概率。          </p><h3 id="马尔可夫"><a href="#马尔可夫" class="headerlink" title="马尔可夫"></a>马尔可夫</h3><p>设状态的历史为 $h_{t}&#x3D;\left{s_{1}, s_{2}, s_{3}, \ldots, s_{t}\right}$ ( $h_t$ 包含了之前的所有状态),如果一个状态转移是符合马尔可夫的，也就是满足如下条件：<br>$$<br>p\left(s_{t+1} \mid s_{t}\right) &#x3D;p\left(s_{t+1} \mid h_{t}\right) \tag{2}<br>$$<br>$$<br>p(s_{t+1}∣s_t,a_t)&#x3D;p(s_{t+1}∣h_t,a_t) \tag{3}<br>$$<br>公式$(2)$表示从当前状态转移到下一个状态和从历史的状态转移到下一个状态时等价的，因此就可以认为下一个状态只跟当前状态有关，跟除当前状态外的其他历史状态无关。公式$(3)$是等价的，只是增加了动作，在之后会涉及到。马尔可夫性质是所有的马尔可夫过程的基础。      </p><h1 id="马尔可夫奖励过程（Markov-Reward-Processes-MRP）"><a href="#马尔可夫奖励过程（Markov-Reward-Processes-MRP）" class="headerlink" title="马尔可夫奖励过程（Markov Reward Processes,MRP）"></a>马尔可夫奖励过程（Markov Reward Processes,MRP）</h1><p><strong>马尔可夫奖励过程</strong>是在马尔科夫链上加了一个奖励函数。<strong>奖励函数R是一个期望</strong>，指当到达某一个状态时能获得多大的奖励。</p><div align="center"><img src="https://datawhalechina.github.io/easy-rl/chapter2/img/2.8.png" width="70%">       </div> <p>上图中，当到达$s_1$时可以获得5的奖励，当到达$s_7$时可以获得10的奖励，则每个点的奖励函数可以用一个矩阵来表示$R&#x3D;\left[ 5,0,0,0,0,0,10 \right]$    </p><h2 id="回报和价值函数"><a href="#回报和价值函数" class="headerlink" title="回报和价值函数"></a>回报和价值函数</h2><p>回报指的是把奖励按照步数进行折扣后所获得的收益，定义如下：<br>$$<br>G_t&#x3D;R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}+\cdots +\gamma ^{T-t-1}R_T \tag{4}<br>$$<br>这里的$\gamma$是指折扣因子，步数越多折扣越大，因为希望得到更多现有的奖励，而未来的奖励就要打折扣，因为有些马尔科夫链是循环无穷步数的。<br>定义回报后就可以再定义一个状态的价值了，就是<strong>状态价值函数</strong>，在MRP中，状态价值函数定义为状态回报的期望，公式如下：</p><p>$$\begin{aligned}<br>V_t\left( s \right) &amp;&#x3D;E\left[ G_t|s_t&#x3D;s \right]\&amp;&#x3D;E\left[ R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}+\cdots +\gamma ^{T-t-1}R_T|s_t&#x3D;s \right]<br>\end{aligned}\tag{5}$$<br>公式$(5)$用期望表示状态价值函数是因为之后的每一步状态不确定，到达其他状态都有一定的概率。   </p><ul><li>例如上图中，在t时刻，当到达$s_2$状态时，一步之后的状态价值为$E[G_t|s_t&#x3D;s_2]&#x3D;0\times0.2+0\times0.4+5\times0.4&#x3D;2$，当前状态一步之后所得到的状态价值也被记为R(t)  。</li></ul><p>那接下来的问题就是，当知道了当前状态后，如何计算它的价值？    </p><h3 id="蒙特卡罗法"><a href="#蒙特卡罗法" class="headerlink" title="蒙特卡罗法"></a>蒙特卡罗法</h3><p>从某一个状态开始，产生很多条轨迹，把每一条轨迹的回报都算出来，取平均来当做该状态的价值。    </p><h3 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h3><p>Bellman Equation的推导和证明过程如下：<br>$$<br>\begin{aligned}<br>V\left( s \right) &amp;&#x3D;E\left[ G_t|s_t&#x3D;s \right]\&amp;&#x3D;E\left[ R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}+\cdots +\gamma ^{T-t-1}R_T|s_t&#x3D;s \right] \&amp;&#x3D;E\left[ R_{t+1}+\gamma E\left[ R_{t+2}+\gamma R_{t+3}+\cdots \right] |s_t&#x3D;s \right]\&amp;&#x3D;E\left[ R_{t+1}|s_t&#x3D;s \right] +\gamma E\left[ R_{t+2}+\gamma R_{t+3}+\cdots |s_t&#x3D;s \right] \<br>&amp;&#x3D;R\left( t \right) +\gamma E\left[ G_{t+1}|s_t&#x3D;s \right]<br>\end{aligned}<br>\tag{6}<br>$$<br>若要继续推导，需要用到概率论的相关知识：<br>$$<br>E\left( X \right) &#x3D;\sum_{A_i}{E\left( X|A_i \right) P\left( A_i \right)}<br>\<br>E\left( X|Y&#x3D;y \right) &#x3D;\sum_{x\in X}{xP\left( X&#x3D;x|Y&#x3D;y \right)}<br>\tag{7}$$<br>继续推导，但这里需要插入一个如下的证明过程：<br>$$\begin{aligned}<br>E\left[ V\left( t+1 \right) |s_t \right] &amp;&#x3D;E\left[ E\left[ G_{t+1}|s_{t+1} \right] |s_t \right] \&amp;&#x3D;E\left[ \sum_{G_{t+1}}{G_{t+1}P\left( G_{t+1}|s_{t+1} \right) |s_t} \right] \&amp;&#x3D;\sum_{s_{t+1}}{\sum_{G_{t+1}}{G_{t+1}P\left( G_{t+1}|s_{t+1},s_t \right) P\left( s_{t+1}|s_t \right)}}<br>\&amp;&#x3D;\frac{\sum_{s_{t+1}}{\sum_{G_{t+1}}{G_{t+1}P\left( G_{t+1}|s_{t+1},s_t \right) P\left( s_{t+1},s_t \right)}}}{P\left( s_t \right)}\&amp;&#x3D;\frac{\sum_{s_{t+1}}{\sum_{G_{t+1}}{G_{t+1}P\left( G_{t+1},s_{t+1},s_t \right)}}}{P\left( s_t \right)}\&amp;&#x3D;\sum_{s_{t+1}}{\sum_{G_{t+1}}{G_{t+1}P\left( G_{t+1},s_{t+1}|s_t \right)}}<br>\&amp;&#x3D;\sum_{G_{t+1}}{G_{t+1}P\left( G_{t+1}|s_t \right)}<br>\&amp;&#x3D;E\left[ G_{t+1}|s_t \right]<br>\end{aligned}<br>\tag{8}$$<br>将证明结论(7)带入(6)中继续推导：<br>$$\begin{aligned}<br>V\left( s \right) &amp;&#x3D;E\left[ G_t|s_t&#x3D;s \right]\&amp;&#x3D;R\left( t \right) +\gamma E\left[ G_{t+1}|s_t&#x3D;s \right] \&amp;&#x3D;R\left( t \right) +\gamma E\left[ V\left( s_{t+1} \right) |s_t&#x3D;s \right] \&amp;&#x3D;R\left( t \right) +\gamma \sum_{s_{t+1}\in S}{P\left( s_{t+1}|s_t \right) V\left( s_{t+1} \right)}<br>\end{aligned}<br>\tag{9}$$<br>所以<strong>Bellman Equation</strong>就为：<br>$$<br>V\left( s \right)&#x3D;R\left( t \right) +\gamma \sum_{s_{t+1}\in S}{P\left( s_{t+1}|s_t \right) V\left( s_{t+1} \right)}\tag{10}<br>$$<br>表示了这个状态和下个状态之间状态价值函数的迭代关系。   </p><h1 id="马尔可夫决策过程（Markov-Decision-Process-MDP"><a href="#马尔可夫决策过程（Markov-Decision-Process-MDP" class="headerlink" title="马尔可夫决策过程（Markov Decision Process ,MDP)"></a>马尔可夫决策过程（Markov Decision Process ,MDP)</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;马尔可夫过程（Markov-Process-MP）&quot;&gt;&lt;a href=&quot;#马尔可夫过程（Markov-Process-MP）&quot; class=&quot;headerlink&quot; title=&quot;马尔可夫过程（Markov Process,MP）&quot;&gt;&lt;/a&gt;马尔可夫过程（Mark</summary>
      
    
    
    
    <category term="知识库" scheme="http://chanforg.github.io/categories/%E7%9F%A5%E8%AF%86%E5%BA%93/"/>
    
    
    <category term="强化学习" scheme="http://chanforg.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>强化学习基本概念</title>
    <link href="http://chanforg.github.io/2022/04/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://chanforg.github.io/2022/04/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</id>
    <published>2022-04-06T12:12:41.000Z</published>
    <updated>2022-04-06T13:39:27.559Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><div align="center"><img src="https://datawhalechina.github.io/easy-rl/chapter1/img/1.1.png" width="50%">       </div><h3 id="keywords"><a href="#keywords" class="headerlink" title="keywords"></a>keywords</h3><ul><li>强化学习（Reinforcement Learning）：Agent可以在与复杂且不确定的Environment进行交互时，尝试使所获得的Reward最大化的计算算法。</li><li>Action: Environment接收到的Agent当前状态的输出。</li><li>State：Agent从Environment中获取到的状态。</li><li>Reward：Agent从Environment中获取的反馈信号，这个信号指定了Agent在某一步采取了某个策略以后是否得到奖励。</li><li>Exploration：在当前的情况下，继续尝试新的Action，其有可能会使你得到更高的这个奖励，也有可能使你一无所有。</li><li>Exploitation：在当前的情况下，继续尝试已知的可以获得最大Reward的过程，即重复执行这个 Action 就可以了。</li><li>深度强化学习（Deep Reinforcement Learning）：不需要手工设计特征，仅需要输入State让系统直接输出Action的一个end-to-end training的强化学习方法。通常使用神经网络来拟合 value function 或者 policy network。</li><li>Full observability、fully observed和partially observed：当Agent的状态跟Environment的状态等价的时候，我们就说现在Environment是full observability（全部可观测），当Agent能够观察到Environment的所有状态时，我们称这个环境是fully observed（完全可观测）。一般我们的Agent不能观察到Environment的所有状态时，我们称这个环境是partially observed（部分可观测）。</li><li>POMDP（Partially Observable Markov Decision Processes）：部分可观测马尔可夫决策过程，即马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 ss，只能知道部分观测值 oo。</li><li>Action space（discrete action spaces and continuous action spaces）：在给定的Environment中，有效动作的集合经常被称为动作空间（Action space），Agent的动作数量是有限的动作空间为离散动作空间（discrete action spaces），反之，称为连续动作空间（continuous action spaces）。</li><li>policy-based（基于策略的）：Agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。</li><li>valued-based（基于价值的）：Agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。</li><li>model-based（有模型结构）：Agent通过学习状态的转移来采取措施。</li><li>model-free（无模型结构）：Agent没有去直接估计状态的转移，也没有得到Environment的具体转移变量。它通过学习 value function 和 policy function 进行决策。</li></ul><h3 id="序列决策过程（Sequential-Decision-Making"><a href="#序列决策过程（Sequential-Decision-Making" class="headerlink" title="序列决策过程（Sequential Decision Making)"></a>序列决策过程（Sequential Decision Making)</h3><p>在跟环境的交互过程中，agent 会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。<strong>所以历史是观测(observation) O、行为 A、奖励R的序列</strong>：<br>$$<br>H_t&#x3D;O_1,R_1,A_1,\cdots ,O_{t-1},R_{t-1},A_{t-1},O_t,A_t<br>$$<br>Agent 在采取当前动作的时候会依赖于它之前得到的这个历史，所以你可以把整个游戏的状态看成关于这个历史的函数：<br>$$<br>S_t&#x3D;f\left( H_t \right)<br>$$</p><span class="label label-primary">状态(state)</span> **s**和<span class="label label-primary">观测(observation)</span>**o**的关系：<ul><li>状态是对世界的完整描述，不会隐藏任何信息；观测是智能体对于世界的观察，可能会遗漏一些信息</li><li>环境有自己的函数 $S_{t}^{e}&#x3D;f^{e}\left(H_{t}\right)$ 来更新状态，在 agent 的内部也有一个函数 $S_{t}^{a}&#x3D;f^{a}\left(H_{t}\right)$ 来更新状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是 <strong>full observability</strong>，就是全部可以观测。换句话说，当 agent 能够观察到环境的所有状态时，我们称这个环境是<span class="label label-primary">完全可观测的(fully observed)</span>。在这种情况下面，强化学习通常被建模成一个 **Markov decision process(MDP)**的问题。在 MDP 中， $O_{t}&#x3D;S_{t}^{e}&#x3D;S_{t}^{a}$。</li><li>但是有一种情况是 agent 得到的观测并不能包含环境运作的所有状态，因为在这个强化学习的设定里面，环境的状态才是真正的所有状态。也就是说当 agent 只能看到部分的观测，我们就称这个环境是<span class="label label-primary">部分可观测的(partially observed)</span>。在这种情况下面，强化学习通常被建模成一个 POMDP 的问题。<strong>部分可观测马尔可夫决策过程(Partially Observable Markov Decision Processes, POMDP)</strong> 是一个马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 s，只能知道部分观测值 o。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。</li></ul><p>更多信息<a href="https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1">请点击</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基础概念&quot;&gt;&lt;a href=&quot;#基础概念&quot; class=&quot;headerlink&quot; title=&quot;基础概念&quot;&gt;&lt;/a&gt;基础概念&lt;/h2&gt;&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://datawhalechina.github.io/e</summary>
      
    
    
    
    <category term="知识库" scheme="http://chanforg.github.io/categories/%E7%9F%A5%E8%AF%86%E5%BA%93/"/>
    
    
    <category term="强化学习" scheme="http://chanforg.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://chanforg.github.io/2022/04/06/hello-world/"/>
    <id>http://chanforg.github.io/2022/04/06/hello-world/</id>
    <published>2022-04-06T02:51:39.148Z</published>
    <updated>2022-04-06T02:51:39.148Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo server<br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo generate<br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>

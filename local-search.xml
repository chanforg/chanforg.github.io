<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>马尔科夫决策过程</title>
    <link href="/2022/04/06/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
    <url>/2022/04/06/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="马尔可夫过程markov-processmp">马尔可夫过程（MarkovProcess,MP）</h1><h2 id="马尔可夫特征">马尔可夫特征</h2>一个状态的下一个状态只取决于当前的状态，而与当前状态之前的状态没有关系，那么这个状态转移的过程就符合<strong>马尔科夫</strong>的。### 马尔科夫链与状态转移矩阵 <strong>马尔科夫链</strong>如下图所示<div data-align="center"><p><img src="https://datawhalechina.github.io/easy-rl/chapter2/img/2.5.png" width="50%"></p></div><p>图中有四种状态 <spanclass="math inline">\(s_1,s_2,s_3,s_4\)</span>,这四种状态之间相互转移，拿<spanclass="math inline">\(s_1\)</span>来说：</p><ul><li><spanclass="math inline">\(s_1\)</span>继续保持在原状态的概率为0.1</li><li><span class="math inline">\(s_1\)</span>转移到<spanclass="math inline">\(s_2\)</span>的概率为0.2</li><li><span class="math inline">\(s_1\)</span>转移到<spanclass="math inline">\(s_4\)</span>的概率为0.7</li></ul><p>所以可以用一个矩阵来描述上图： <span class="math display">\[P=\left[ \begin{matrix}    \begin{array}{c}    P\left( s_1|s_1 \right)\\    P\left( s_1|s_2 \right)\\    P\left( s_1|s_3 \right)\\    P\left( s_1|s_4 \right)\\\end{array}&amp;        \begin{array}{c}    P\left( s_2|s_1 \right)\\    P\left( s_2|s_2 \right)\\    P\left( s_2|s_3 \right)\\    P\left( s_2|s_4 \right)\\\end{array}&amp;        \begin{array}{c}    P\left( s_3|s_1 \right)\\    P\left( s_3|s_2 \right)\\    P\left( s_3|s_3 \right)\\    P\left( s_3|s_4 \right)\\\end{array}&amp;        \begin{array}{c}    P\left( s_4|s_1 \right)\\    P\left( s_4|s_2 \right)\\    P\left( s_4|s_3 \right)\\    P\left( s_4|s_4 \right)\\\end{array}\\\end{matrix} \right] =\left[ \begin{matrix}    0.1&amp;        0.2&amp;        0&amp;      0.7\\    1&amp;      0&amp;      0&amp;      0\\    0&amp;      1&amp;      0&amp;      0\\    0&amp;      0.3&amp;        0.2&amp;        0.5\\\end{matrix} \right] \tag{1}\]</span><br /><span class="math inline">\((1)\)</span>就称为<strong>状态转移矩阵</strong>，矩阵的每一行都描述了当到达某个节点后，在下一步转移到其他节点的概率。<br />### 马尔可夫 设状态的历史为<spanclass="math inline">\(h_{t}=\left\{s_{1}, s_{2}, s_{3}, \ldots,s_{t}\right\}\)</span>(<spanclass="math inline">\(h_t\)</span>包含了之前的所有状态),如果一个状态转移是符合马尔可夫的，也就是满足如下条件：<span class="math display">\[p\left(s_{t+1} \mid s_{t}\right) =p\left(s_{t+1} \mid h_{t}\right)\tag{2}\]</span> <span class="math display">\[p(s_{t+1}∣s_t,a_t)=p(s_{t+1}∣h_t,a_t) \tag{3}\]</span> 公式<spanclass="math inline">\((2)\)</span>表示从当前状态转移到下一个状态和从历史的状态转移到下一个状态时等价的，因此就可以认为下一个状态只跟当前状态有关，跟除当前状态外的其他历史状态无关。公式<spanclass="math inline">\((3)\)</span>是等价的，只是增加了动作，在之后会涉及到。马尔可夫性质是所有的马尔可夫过程的基础。</p><h1id="马尔可夫奖励过程markov-reward-processesmrp">马尔可夫奖励过程（MarkovReward Processes,MRP）</h1><h1id="马尔可夫决策过程markov-decision-process-mdp">马尔可夫决策过程（MarkovDecision Process ,MDP)</h1>]]></content>
    
    
    <categories>
      
      <category>知识库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习基本概念</title>
    <link href="/2022/04/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <url>/2022/04/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    
    <content type="html"><![CDATA[<h2 id="基础概念">基础概念</h2><div data-align="center"><p><img src="https://datawhalechina.github.io/easy-rl/chapter1/img/1.1.png" width="50%"></p></div><h3 id="keywords">keywords</h3><ul><li>强化学习（ReinforcementLearning）：Agent可以在与复杂且不确定的Environment进行交互时，尝试使所获得的Reward最大化的计算算法。</li><li>Action: Environment接收到的Agent当前状态的输出。</li><li>State：Agent从Environment中获取到的状态。</li><li>Reward：Agent从Environment中获取的反馈信号，这个信号指定了Agent在某一步采取了某个策略以后是否得到奖励。</li><li>Exploration：在当前的情况下，继续尝试新的Action，其有可能会使你得到更高的这个奖励，也有可能使你一无所有。</li><li>Exploitation：在当前的情况下，继续尝试已知的可以获得最大Reward的过程，即重复执行这个Action 就可以了。</li><li>深度强化学习（Deep ReinforcementLearning）：不需要手工设计特征，仅需要输入State让系统直接输出Action的一个end-to-endtraining的强化学习方法。通常使用神经网络来拟合 value function 或者policy network。</li><li>Full observability、fully observed和partiallyobserved：当Agent的状态跟Environment的状态等价的时候，我们就说现在Environment是fullobservability（全部可观测），当Agent能够观察到Environment的所有状态时，我们称这个环境是fullyobserved（完全可观测）。一般我们的Agent不能观察到Environment的所有状态时，我们称这个环境是partiallyobserved（部分可观测）。</li><li>POMDP（Partially Observable Markov DecisionProcesses）：部分可观测马尔可夫决策过程，即马尔可夫决策过程的泛化。POMDP依然具有马尔可夫性质，但是假设智能体无法感知环境的状态ss，只能知道部分观测值 oo。</li><li>Action space（discrete action spaces and continuous actionspaces）：在给定的Environment中，有效动作的集合经常被称为动作空间（Actionspace），Agent的动作数量是有限的动作空间为离散动作空间（discrete actionspaces），反之，称为连续动作空间（continuous action spaces）。</li><li>policy-based（基于策略的）：Agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。</li><li>valued-based（基于价值的）：Agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。</li><li>model-based（有模型结构）：Agent通过学习状态的转移来采取措施。</li><li>model-free（无模型结构）：Agent没有去直接估计状态的转移，也没有得到Environment的具体转移变量。它通过学习value function 和 policy function 进行决策。</li></ul><h3 id="序列决策过程sequential-decision-making">序列决策过程（SequentialDecision Making)</h3><p>在跟环境的交互过程中，agent会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。<strong>所以历史是观测(observation)O、行为 A、奖励R的序列</strong>： <span class="math display">\[H_t=O_1,R_1,A_1,\cdots ,O_{t-1},R_{t-1},A_{t-1},O_t,A_t\]</span> Agent在采取当前动作的时候会依赖于它之前得到的这个历史，所以你可以把整个游戏的状态看成关于这个历史的函数：<span class="math display">\[S_t=f\left( H_t \right)\]</span> <span class="label label-primary">状态(state)</span><strong>s</strong>和<span class="label label-primary">观测(observation)</span><strong>o</strong>的关系：</p><ul><li>状态是对世界的完整描述，不会隐藏任何信息；观测是智能体对于世界的观察，可能会遗漏一些信息</li><li>环境有自己的函数 <spanclass="math inline">\(S_{t}^{e}=f^{e}\left(H_{t}\right)\)</span>来更新状态，在 agent 的内部也有一个函数 <spanclass="math inline">\(S_{t}^{a}=f^{a}\left(H_{t}\right)\)</span>来更新状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是<strong>full observability</strong>，就是全部可以观测。换句话说，当agent能够观察到环境的所有状态时，我们称这个环境是<span class="label label-primary">完全可观测的(fully observed)</span>。在这种情况下面，强化学习通常被建模成一个<strong>Markov decision process(MDP)</strong>的问题。在 MDP 中， <spanclass="math inline">\(O_{t}=S_{t}^{e}=S_{t}^{a}\)</span>。</li><li>但是有一种情况是 agent得到的观测并不能包含环境运作的所有状态，因为在这个强化学习的设定里面，环境的状态才是真正的所有状态。也就是说当agent只能看到部分的观测，我们就称这个环境是<span class="label label-primary">部分可观测的(partially observed)</span>。在这种情况下面，强化学习通常被建模成一个POMDP 的问题。<strong>部分可观测马尔可夫决策过程(Partially ObservableMarkov Decision Processes, POMDP)</strong>是一个马尔可夫决策过程的泛化。POMDP依然具有马尔可夫性质，但是假设智能体无法感知环境的状态s，只能知道部分观测值o。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。</li></ul><p>更多信息<ahref="https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1">请点击</a></p>]]></content>
    
    
    <categories>
      
      <category>知识库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/04/06/hello-world/"/>
    <url>/2022/04/06/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></div></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo server<br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo generate<br></code></pre></div></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></div></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
